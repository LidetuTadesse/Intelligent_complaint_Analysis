{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95434ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#****************************** Step1 ********************************\n",
    "# Imporing Standard and third-party imports for RAG pipeline\n",
    "import pandas as pd\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d427cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************ Step 2 *********************************************************\n",
    "# *********************** Load FAISS Vectore Store and Embedding Model *******************\n",
    "\n",
    "# Load the same embedding model as used in Task 2\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load the vector store built in Task 2\n",
    "vector_db = FAISS.load_local(\n",
    "    \"../notebooks/vector_store\",  \n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092f3ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************* Step 3 *******************************************\n",
    "# ********************************Define Retrival Function ***************************\n",
    "\n",
    "def retrieve_top_chunks(query: str, k: int = 5):\n",
    "    \"\"\"\n",
    "    Retrieves the top-k most relevant complaint chunks from FAISS index.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's question.\n",
    "        k (int): Number of chunks to retrieve.\n",
    "        \n",
    "    Returns:\n",
    "        List of Document objects.\n",
    "    \"\"\"\n",
    "    return vector_db.similarity_search(query, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faa488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***************************************  Step 4 *********************************\n",
    "#***************************************  Define RAG Prompt Template *************\n",
    "\n",
    "rag_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a financial analyst assistant at CrediTrust.\n",
    "Your task is to answer customer complaint-related questions using only the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer (based only on the context above):\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b81f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************* Step 5 ****************************************\n",
    "# ************************ Load a Lightweight LLM ************************\n",
    "\n",
    "# We'll use HuggingFace's DistilBERT for generation (very lightweight and CPU-friendly)\n",
    "\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"distilgpt2\",  # This can be switch to mistral, llama, etc. concidering available resource\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e420f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************** Step 6 **************************\n",
    "#********************* Combine Retriever + LLM into RetrievalQA Chain ********************\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vector_db.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26202607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************* Step 7 *********************************************\n",
    "# ************************* Define Example Queries for Evaluation **************\n",
    "\n",
    "example_questions = [\n",
    "    \"Why are users unhappy with Buy Now, Pay Later services?\",\n",
    "    \"What issues do customers report about savings accounts?\",\n",
    "    \"Are there any complaints about money transfers being delayed?\",\n",
    "    \"Why do people dislike credit cards?\",\n",
    "    \"What is the most common complaint about personal loans?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c942e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************** Step 8 *********************************************\n",
    "# ************************** Run Evaluation and Store Results *******************\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for question in example_questions:\n",
    "    result = rag_chain({\"query\": question})\n",
    "    retrieved_docs = retrieve_top_chunks(question)\n",
    "    \n",
    "    evaluation_results.append({\n",
    "        \"Question\": question,\n",
    "        \"Generated Answer\": result[\"result\"],\n",
    "        \"Retrieved Source 1\": retrieved_docs[0].page_content[:250],  # Short preview\n",
    "        \"Retrieved Source 2\": retrieved_docs[1].page_content[:250] if len(retrieved_docs) > 1 else \"\",\n",
    "        \"Quality Score (1-5)\": \"\",  # Leave blank to score manually\n",
    "        \"Comments\": \"\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35104077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********************* Step 9 ********************************************************\n",
    "# ********************* Display Evaluation Table for Manual Review ********************\n",
    "\n",
    "eval_df = pd.DataFrame(evaluation_results)\n",
    "eval_df[[\"Question\", \"Generated Answer\", \"Retrieved Source 1\", \"Retrieved Source 2\", \"Quality Score (1-5)\", \"Comments\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f822a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d684349b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
