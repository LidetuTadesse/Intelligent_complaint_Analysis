{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e95434ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ltlid\\OneDrive\\Desktop\\Educational\\10 Academy\\Week 6\\Intelligent_complain_analysis\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#****************************** Step1 ********************************\n",
    "# Imporing Standard and third-party imports for RAG pipeline\n",
    "import pandas as pd\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d427cc51",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error in __cdecl faiss::FileIOReader::FileIOReader(const char *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\impl\\io.cpp:68: Error: 'f' failed: could not open ..\\notebooks\\vector_store\\index.faiss for reading: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_16592\\3379221014.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load the same embedding model as used in Task 2\u001b[39;00m\n\u001b[32m      5\u001b[39m embedding_model = HuggingFaceEmbeddings(model_name=\u001b[33m\"sentence-transformers/all-MiniLM-L6-v2\"\u001b[39m)\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Load the vector store built in Task 2\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m vector_db = FAISS.load_local(\n\u001b[32m      9\u001b[39m     \u001b[33m\"../notebooks/vector_store\"\u001b[39m,\n\u001b[32m     10\u001b[39m     embedding_model,\n\u001b[32m     11\u001b[39m     allow_dangerous_deserialization=\u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\ltlid\\OneDrive\\Desktop\\Educational\\10 Academy\\Week 6\\Intelligent_complain_analysis\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(cls, folder_path, embeddings, index_name, allow_dangerous_deserialization, **kwargs)\u001b[39m\n\u001b[32m   1201\u001b[39m             )\n\u001b[32m   1202\u001b[39m         path = Path(folder_path)\n\u001b[32m   1203\u001b[39m         \u001b[38;5;66;03m# load index separately since it is not picklable\u001b[39;00m\n\u001b[32m   1204\u001b[39m         faiss = dependable_faiss_import()\n\u001b[32m-> \u001b[39m\u001b[32m1205\u001b[39m         index = faiss.read_index(str(path / f\"{index_name}.faiss\"))\n\u001b[32m   1206\u001b[39m \n\u001b[32m   1207\u001b[39m         \u001b[38;5;66;03m# load docstore and index_to_docstore_id\u001b[39;00m\n\u001b[32m   1208\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m open(path / f\"{index_name}.pkl\", \u001b[33m\"rb\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[32mc:\\Users\\ltlid\\OneDrive\\Desktop\\Educational\\10 Academy\\Week 6\\Intelligent_complain_analysis\\.venv\\Lib\\site-packages\\faiss\\swigfaiss_avx2.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m  11140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m read_index(*args):\n\u001b[32m> \u001b[39m\u001b[32m11141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _swigfaiss_avx2.read_index(*args)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error in __cdecl faiss::FileIOReader::FileIOReader(const char *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\impl\\io.cpp:68: Error: 'f' failed: could not open ..\\notebooks\\vector_store\\index.faiss for reading: No such file or directory"
     ]
    }
   ],
   "source": [
    "#************************ Step 2 *********************************************************\n",
    "# *********************** Load FAISS Vectore Store and Embedding Model *******************\n",
    "\n",
    "# Load the same embedding model as used in Task 2\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load the vector store built in Task 2\n",
    "vector_db = FAISS.load_local(\n",
    "    \"../notebooks/vector_store\",  \n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092f3ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************* Step 3 *******************************************\n",
    "# ********************************Define Retrival Function ***************************\n",
    "\n",
    "def retrieve_top_chunks(query: str, k: int = 5):\n",
    "    \"\"\"\n",
    "    Retrieves the top-k most relevant complaint chunks from FAISS index.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's question.\n",
    "        k (int): Number of chunks to retrieve.\n",
    "        \n",
    "    Returns:\n",
    "        List of Document objects.\n",
    "    \"\"\"\n",
    "    return vector_db.similarity_search(query, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faa488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***************************************  Step 4 *********************************\n",
    "#***************************************  Define RAG Prompt Template *************\n",
    "\n",
    "rag_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a financial analyst assistant at CrediTrust.\n",
    "Your task is to answer customer complaint-related questions using only the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer (based only on the context above):\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b81f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************* Step 5 ****************************************\n",
    "# ************************ Load a Lightweight LLM ************************\n",
    "\n",
    "# We'll use HuggingFace's DistilBERT for generation (very lightweight and CPU-friendly)\n",
    "\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"distilgpt2\",  # This can be switch to mistral, llama, etc. concidering available resource\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e420f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************** Step 6 **************************\n",
    "#********************* Combine Retriever + LLM into RetrievalQA Chain ********************\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vector_db.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26202607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************* Step 7 *********************************************\n",
    "# ************************* Define Example Queries for Evaluation **************\n",
    "\n",
    "example_questions = [\n",
    "    \"Why are users unhappy with Buy Now, Pay Later services?\",\n",
    "    \"What issues do customers report about savings accounts?\",\n",
    "    \"Are there any complaints about money transfers being delayed?\",\n",
    "    \"Why do people dislike credit cards?\",\n",
    "    \"What is the most common complaint about personal loans?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c942e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************** Step 8 *********************************************\n",
    "# ************************** Run Evaluation and Store Results *******************\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for question in example_questions:\n",
    "    result = rag_chain({\"query\": question})\n",
    "    retrieved_docs = retrieve_top_chunks(question)\n",
    "    \n",
    "    evaluation_results.append({\n",
    "        \"Question\": question,\n",
    "        \"Generated Answer\": result[\"result\"],\n",
    "        \"Retrieved Source 1\": retrieved_docs[0].page_content[:250],  # Short preview\n",
    "        \"Retrieved Source 2\": retrieved_docs[1].page_content[:250] if len(retrieved_docs) > 1 else \"\",\n",
    "        \"Quality Score (1-5)\": \"\",  # Leave blank to score manually\n",
    "        \"Comments\": \"\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35104077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********************* Step 9 ********************************************************\n",
    "# ********************* Display Evaluation Table for Manual Review ********************\n",
    "\n",
    "eval_df = pd.DataFrame(evaluation_results)\n",
    "eval_df[[\"Question\", \"Generated Answer\", \"Retrieved Source 1\", \"Retrieved Source 2\", \"Quality Score (1-5)\", \"Comments\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f822a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d684349b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
